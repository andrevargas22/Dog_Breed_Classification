<!doctype html>
<head>
	<link rel= "stylesheet" type= "text/css" href= "{{ url_for('static',filename='styles/main.css') }}">
	<title>Dog Breed Classifier - Report</title>
	<link rel="shortcut icon" href="{{ url_for('static', filename='favicon.ico') }}">
</head>

<h1> Dog Breed Classifier with PyTorch, by André Vargas</h1>

<h2> Project Overview:</h2>

<h3>    - What is Machine Learning? </h3>

<p>
	On modern days, the algorithms are all around us. Language assistants that instantly
	translates from one language to another, self-driving cars, personalized movie suggestions in
	your favourite streaming platform, personalized shopping suggestions and automatic price
	setting, diagnosis of a disease based on collected data. All these tasks that a few years ago
	would be science fiction are possible today thanks to Machine Learning algorithms. A
	technology that allow computers and other devices to learn and solve different tasks by
	themselves without explicitly programming them how to do so.
</p>

<p>
	The Collins English Dictionary defines Intelligence as “the ability to think, reason, and
	understand instead of doing things automatically or by instinct” [1]. For example, before
	lifting a vase we are able to predict its weight based on our intelligence, and know whether or
	not we are able to do it. This reasoning to predict certain situations is possible because of our
	memory from previous experiences, and it’s exactly the same principles applied to Machine
	Learning.
</p>

<p>
	Computers can learn trough associations of different types of data, which can be numbers,
	images, patterns, classes, and so on. And beyond a massive amount of problems to solve,
	image classification is one of the biggest areas of study in the domain of Machine Learning.
</p>

<h3>    - What is Image Classification? </h3>

<p>
	There are hundreds of applications for image classification with Machine Learning. For
	example, let’s say we want a machine to analyse pictures of recent harvested strawberries to
	see which of them are good or bad for humans to eat, or let's imagine a doctor that uses a
	software capable of looking at an x-ray image and labeling as cancer or not.
</p>

<p>
	Regardless of being a simple classification problem with two categories, good strawberries
	and bad strawberries, or a more complex one, they are easy problems for humans simply because
	we just know in our brain how a good strawberry looks and how a bad strawberry looks, but it’s
	impossible to tell a computer how to do it. And one of the solutions is to use Machine Learning
	algorithms to teach the machines how to identify specific patterns in these images
	that allow them to be classified.
</p>

<br>

<div id="report">
	<img src="/static/report_images/strawberries.PNG">
</div>
<small>Image 1: A bad and a good strawberry [2]</small>

<br><br>

<h2> Problem Statement:</h2>

<p>
	The goal of this project is to use an Image Classification algorithm to identify dog breeds
	from image inputs. This is a very complex problem because dogs are very similar among them in therms of body
	features and overall structure.
</p>

<p>
	Dogs are one of the most diverse species on the planet, there are over 340 dog breeds known
	around the world. The American Kennel Club recognize over 170 official dog breeds, not
	counting mixed breeds and mutts.[3] Differentiating between all these categories is not an
	easy task for a machine to do, and a very large dataset is required.
</p>

<br>

<div>
	<img src="/static/report_images/similar_dogs.PNG">
</div>
<small>Image 2: Can you tell which one is a German Shepherd Dog and which one is a Belgian Malinois? [4]</small>

<br><br>

<p>
	The basic premise of this project is that given an image of a dog, the algorithm must identify an estimate of
	the canine’s breed. If supplied an image of a human, the code will identify the resembling
	dog breed. Pretty straight forward. We will create a small web application running in Flask that asks an
	input image from the user and returns the breed contained in that image.
</p>

<p> This is the final project for Udacity's Machine Learning Engineer Nanodegree and
	all steps to accomplish this task will be covered on this report.
</p>

<h2>Data Exploration</h2>

<p>
	Before discussing any algorithm possibilities, let's take a look at the dataset available for this project. Since it's an
	image classification problem, it's not surpring that the dataset is made of lots and lots of images. We will be using two
	datasets to accomplish this project. Both of them were provided by Udacity and can be downloaded in the links below.
</p>

<h3> - Dog Dataset:</h3>
<p>
	There are a total of 8351 dog images separated in train (6680 images), test (836 images) and valid (835 images),
	and each group separated in 133 different dog breeds. The dataset can be downloaded
	<a href="https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip">here</a>.
	It's interesting to note how balanced the classes are within each sub-dataset. This will certainly help
	to improve the algorithm results.
</p>

<br>

<div>
	<img src="/static/report_images/classes.png">
</div>
<small>Image 3: All 133 classes available in the dataset.</small>

<br><br>

<p>
	Let's take a look at some examples from the dog dataset:
</p>

<br>

<div>
	<img src="/static/report_images/dog_samples.PNG">
</div>
<small>Image 4: Classes 072.German_shorthaired_pointer and 099.Lhasa_apso.</small>

<br><br>

<h3> - Human Dataset:</h3>
<p>
	And since this project should be able to identify and give a breed to a human image, there are 13233 human images
	as well which will be used for testing. This dataset can be downloaded
	<a href="http://vis-www.cs.umass.edu/lfw/lfw.tgz">here</a>.
</p>

<p>
	Some examples from the human dataset:
</p>

<br>

<div>
	<img src="/static/report_images/human_samples.PNG">
</div>
<small>Image 5: Harrison Ford and Roger Federer.</small>

<br><br>

<h2> Algorithms and Techniques </h2>

<p>
	There are several Machine Learning algorithms to deal with image processing and image classification,
	but considering the fact that we are expecting to classify dogs with several specificities, a good
	approach is to use one that identify specific patterns in these images that allow them to be classified.
	One way to do this job is using a Convolutional Neural Network (CNN).
</p>

<h3>- What is a Convolutional Neural Network?</h3>

<p>
	A CNN is a specific type of neural network and its largely used for image classification. To
	understand a little bit better how a CNN works, it helps if we look at how the computers
	“see” an image.
</p>

<p>
	A black and white image is seen by the computer as a 2D matrix, and each position of this
	matrix represents a pixel of this image. The values for each element vary between 0 (black)
	and 255 (white). This can be seen in the example below:
</p>

<br>

<div>
	<img src="/static/report_images/digital_image.PNG">
</div>
<small>Image 6: Creation of a Digital Image [5]</small>

<br><br>

<p>
	A coloured image, on the other hand, is represented by a 3D matrix in which we can store a
	combination of the colours red, green and blue (RGB) from 0 to 255. The image below in an
	example of that:
</p>

<br>

<div>
	<img src="/static/report_images/coloured_image.PNG">
</div>
<small>Image 7: Digitalization of a coloured picture [6]</small>

<br><br>

<p>
	So, considering that a machine can only see groups of numbers, one way to analyse and
	determine whether that image is a “cat” or a “dog” is by looking for specific patterns and
	characteristics for each image class. These operation of “reading” the matrices showed above,
	assign some weights and biases to some aspects of the image, and based on that differentiate
	the images is the basic premise of a CNN. [7]
</p>

<p> This is obviously an oversimplification, there is a lot of complexity to add to it, but essentially
	a CNN can be seen as a class of neural networks in which a series of operations are performed from one
	layer to another until an output is given. They extract features from images.
</p>

<br>

<div>
	<img src="/static/report_images/CNN_scheme.png">
</div>
<small>Image 8: Basic CNN scheme [8]</small>

<br><br>

<p>
	CNNs are also used in a variety of other fields like face recognition, document analysis,
	understanding climate changes, speech recognition, and so on. [9]
</p>

<p>
	In order to create and test our CNNs, we will be using PyTorch. PyTorch is a Python-based library
	that provides functionalities such as:
</p>

<p>
	- Creating serializable and optimizable models;<br>
	- Distributed training to parallelize computations;<br>
	- Dynamic Computation graphs which enable to make the computation graphs on the go, and many more.<br>
</p>

<p>
	PyTorch tensors in general are similar to NumPy’s n-dimensional arrays which can also be used with GPUs.
	Performing operations on these tensors is almost similar to performing operations on NumPy arrays.
	This makes PyTorch very user-friendly and easy to learn.
</p>

<h2> Benchmark </h2>

<p>
	In this project, we will test two different CNNs: the first one created from scratch, making the choices
	for each layer, each configuration, and defining the behaviour of the forward function, and the second CNN
	will be created using transfer learning. Transfer learning is a very simple concept: A model trained on a large
	dataset has its knowledge transfered to a smaller dataset. This allow us to use a network trained in a massive
	dataset and apply it to our problem, expecting a better performance.
</p>

<p>
	So, given this idea, our benchmark model will be the CNN created from scratch, and our goal is to achieve
	an accuracy of 40%.
</p>

<p>
	As for the transfer learning CNN, the goal is to achieve an accuracy of at least 85%, and it will be the model
	tested and used on the web application.
</p>

<h2>Implementation</h2>
<p>
	We will split the implementation of this project into 7 separated steps:<br><br>
	Step 1) Import the datasets;<br>
	Step 2) Create a human face detector;<br>
	Step 3) Create a dog detector;<br>
	Step 4) Preprocess the data;<br>
	Step 5) Create a CNN to classify dog breeds from scratch;<br>
	Step 6) Create a CNN to classify dog breeds using transfer learning;<br>
	Step 7) Write the full algorithm;<br>
</p>

<p>
	Let's go through all these steps, in detail, one by one.
</p>

<h3>- Step 0: Import the datasets</h3>

<p>
	After downloading both datasets from the links provided before, we save the file paths for both the human (LFW)
	dataset and dog dataset in the numpy arrays human_files and dog_files. This will make testing much easier later.
</p>

<br>

<div>
	<img src="/static/report_images/import.png">
</div>
<small>Image 9: Importing Libraries</small>

<h3>- Step 1: Create a human face detector</h3>

<p>
	Since we are supposed to identify when a human is present in the image, we can use OpenCV's
	implementation of Haar feature-based cascade classifiers to detect human faces in images.
	OpenCV provides many pre-trained face detectors stored as XML files on GitHub. We have
	downloaded one of these detectors and are going to use on this project. The XML file can be downloaded
	from this project repository on
	<a href="https://github.com/andrevargas22/Dog_Breed_Classification">Github</a>.
</p>

<p>
	Let's take a look on how to use this detector to find human faces in a sample image.
</p>

<br>

<div>
	<img src="/static/report_images/opencv.PNG">
</div>
<div>
	<img src="/static/report_images/face.png">
</div>
<small>Image 10: Detecting a face with OpenCV</small>

<p>
	In the above code, faces is a numpy array of detected faces, where each row corresponds to a detected face.
	Which means that if we check the lenght of faces, we can tell if a face has been detected or not, in other words,
	a function named face_detector that takes a string-valued file path to an image as input and returns
	True or False, depending on the result.
</p>
<div>
	<img src="/static/report_images/face_detector.PNG">
</div>
<small>Image 11: Face detector function</small>

<p>
	Now, as a side mission, let's test the performance of the face_detector function. Ideally, we would like 100%
	of human images with a detected face and 0% of dog images with a detected face. The algorithm falls short of
	this goal, but still gives acceptable performance. We extract the file paths for the first 100 images from
	each of the datasets and store them in the numpy arrays human_files_short and dog_files_short.
</p>

<br>

<div>
	<img src="/static/report_images/face_detector_perfomance.PNG">
</div>
<small>Image 12: Face detector performance</small>

<p>
	As we've already mentioned, it's not perfect, and it’s interesting trying to understand why. Some of pictures from
	the dog dataset have humans in it, so the function will return true because it’s not necessarily a dog only image.
	Like this example:
</p>

<div>
	<img src="/static/report_images/human_and_dog.PNG">
</div>
<small>Image 13: From the dataset "dog_images/train/103.Mastiff"</small>

<p>
	In other cases, the algorithm will simply identify some human features in specific patterns of the image and will consider as a face.
</p>

<div>
	<img src="/static/report_images/what.png">
</div>
<small>Image 14: From the dataset</small>

<p> And in the human dataset, there are a few images with lower quality that the classifier was unable to understand:</p>

<div>
	<img src="/static/report_images/humanwhat.png">
</div>
<small>Image 15: From the dataset</small>

<p>
	But still, the results were very acceptable.
</p>

<p>
	There are more face algorithms out there, and we can take a look at another one to compare the results with OpenCV. Let's check the performance of
	the Multi-Task Cascaded Convolutional Neural Network (MTCNN)[10], a CNN by itself.
</p>

<p>
	MTCNN stores the attributes of any faces detected in a list, as it can be seen from the example below, which has 8 faces.
</p>

<div>
	<img src="/static/report_images/mtcnn.PNG">
</div>
<div>
	<img src="/static/report_images/mtcnnfaces.PNG">
</div>
<small>Image 16: MTCNN detection</small>

<p>
	So, using the same logic from our face_detector function, a face_detector using MTCNN could be done by reading the result list after looking an image,
	if there are any attributes stored, there is a face detected.
</p>

<div>
	<img src="/static/report_images/face_detectorMTCNN.PNG">
</div>

<small>Image 17: MTCNN face_detector function</small>

<p>And again, running in a sample of both datasets, we can see how well this model performs.</p>

<div>
	<img src="/static/report_images/mtcnn_performance.PNG">
</div>
<small>Image 18: MTCNN performance</small>

<p>MTCNN was able to identify all humans in the human dataset sample, but found more faces in the dog dataset than the OpenCV method.</p>

<div>
	<img src="/static/report_images/comparative_face.PNG">
</div>
<small>Image 19: Comparative Table</small>

<p> This concludes Step 2 of this project, we will be using OpenCV's face_detector function later in the final code.</p>

<h3>- Step 3: Create a dog detector</h3>

<p>
	In this section, we will use a pre-trained model to detect dogs in images.
</p>

<p>
	The code cell below downloads the VGG-16 model, along with weights that have been trained on ImageNet, a very large, very popular dataset
	used for image classification and other vision tasks. ImageNet contains over 10 million URLs, each linking to an image containing an object
	from one of 1000 categories.
</p>
<div>
	<img src="/static/report_images/importVGG16.PNG">
</div>
<small>Image 20: Downloading VGG-16 model</small>

<p>
	The ideia is simple, given an image, this pre-trained VGG-16 model returns a prediction (an integer from 0 to 999
	derived from the 1000 possible categories in ImageNet) for the object that is contained in the image. If we take a look at ImageNet categories,
	we'll see that categories corresponding to dogs appear in an uninterrupted sequence and correspond to dictionary keys 151-268, inclusive, to include
	all categories from 'Chihuahua' to 'Mexican hairless'. Thus, to check to see if an image is predicted to contain a dog by the pre-trained VGG-16 model,
	we need only check if the pre-trained model predicts an index between 151 and 268 (inclusive).
</p>

<p> In order to write the dog_detector function, it's important to take a look at PyTorch's

	<a href="https://pytorch.org/vision/stable/models.html">documentation</a>

	to understand how to appropriately pre-process tensors for pre-trained models
</p>

<p>
	In the code cell bellow, we have a little spoiler of what kind of preprocessing we're gonna have to do to our dataset when we train the model.
	We'll discuss these steps with more detail on Step 4. The relevant information is that the function reads an image from its path, preprocess it,
	convert to a tensor accepted by the model, and returns the index prediction of what that image is. We tested on a golden retriever image and got
	the result 207, which is golden retriever in ImageNet classification.
</p>
<div>
	<img src="/static/report_images/VGG16predict.png">
</div>
<div>
	<img src="/static/report_images/VGG16result.PNG">
</div>
<div>
	<img src="/static/report_images/golden.PNG">
</div>
<small>Image 21: VGG-16 prediction test</small>

<p>
	Now with the VGG16_predict funcion, to wrap up our dog_detector, all we have to do is create a logic that gives True if the predicted index is
	between 151 and 268 (inclusive), and False for every other value.
</p>
<div>
	<img src="/static/report_images/dog_detector.PNG">
</div>
<small>Image 22: dog_detector function test</small>

<p>
	Now we can test the performance in the first 100 images of each dataset, like we did with the face_detector function.
</p>
<div>
	<img src="/static/report_images/dog_detector_perfomance.PNG">
</div>

<p>
	It works perfectly.
</p>

<p>
	As an exercise, let's try a different dog detector using the ResNet50 pre-trained model[11].
</p>

<div>
	<img src="/static/report_images/dog_detectorRESNET.PNG">
</div>

<div>
	<img src="/static/report_images/resnet50.PNG">
</div>
<small>Image 23: dog_detector function using ResNet50 </small>

<p>
	We can test its performance and compare to VGG-16.
</p>

<div>
	<img src="/static/report_images/resnet50performance.PNG">
</div>
<small>Image 23: ResNet50 dog_detector performance </small>

<br><br>
<div>
	<img src="/static/report_images/comparative_dogs.PNG">
</div>
<small>Image 24: Comparative Table </small>

<p>
	ResNet50 also works perfectly to our needs, but it's a lot slower than VGG-16. Therefore, we'll keep our dog_detector function
	with VGG-16, which will be used later in the final algorithm. This concludes Step 3 of this project.
</p>

<h3>- Step 4: Preprocess the data</h3>

<p>
	The first thing we need to do is to write three separate data loaders for the training, validation, and test datasets of dog images.
	A Data loader combines a dataset and a sampler, and provides an iterable over the given dataset. This is where the preprocessing of the
	data takes place.
</p>

<p>
	We've already had a glimpse of what needs to be done on this step. On the definition of VGG16_predict function, we needed to do some processing
	of the image in order to convert to an acceptable tensor, so the pre-trained model could understand and give a valid output. This is exactly what
	we're going to do to create our dataloaders.
</p>

<p>
	Before we start going through this process, we need to introduce the concept of data augmentation.
</p>

<p>
	When training a machine learning model, what is happening behind the curtains is a tuning of its parameters in a way that it can map a particular input
	(an image of a dog) to some output (its breed). The optimization goal is to chase that sweet spot where the model’s loss is low, which happens the parameters
	are tuned in the right way. But if there are a lot of parameters, we would need to show the machine learning model a proportional amount of examples
	to get good performance. Also, the number of parameters needed is proportional to the complexity of the task the model has to perform.
</p>

<p>
	But do we get more data, if I don’t have “more data”? This is when data augmentation shows up. It's not necessary to hunt for novel new images that can be
	added to your dataset because neural networks understands minor alterations to an image such as flips, translations or rotations as a totally new and unique image.
	So, to get more data, we just need to make small changes to our existing dataset. That is the basic idea behind data augmentation.
</p>

<p>
	Luckily, PyTorch has a model called transforms, which allows us to perform different transformations to our images.
	By using the class transforms.Compose, we can chain several transforms together. So now, let's finally discuss how we're going to preprocess
	our dataset.
</p>

<p>
	For the train dataset, we are going to chain these transformations: <br>
	- Random Resized Crop(224) - Crop a random portion of image and resize it to 224x224, which is the size of the tensor accepted by the model;<br>
	- Random Horizontal Flip() - Horizontally flip the given image randomly with a given probability. This was a choice made to augment our data.<br>
	- Random Rotation (15) - Rotate the image by angle of 15°. Another choice made to augment the data.<br>
	- To Tensor() - Convert a PIL Image or numpy.ndarray to tensor.<br>
    - Normalize a float tensor image with mean and standard deviation.<br>
</p>

<p>
	For the valid dataset, we are going to chain these transformations: <br>
	- Random Resize(256) - Resize the input image to the given size.
    - Center Crop (224) - Crops the given image at the center with the given size (224 is the size of tensor accepted by the model).<br>
    - To Tensor() - Convert a PIL Image or numpy.ndarray to tensor.<br>
    - Normalize a float tensor image with mean and standard deviation.<br>
</p>

<p>
	For the test dataset, we are going to chain these transformations: <br>
	- Random Resize(256) - Resize the input image to the given size.
    - Center Crop (224) - Crops the given image at the center with the given size (224 is the size of tensor accepted by the model).<br>
    - To Tensor() - Convert a PIL Image or numpy.ndarray to tensor.<br>
    - Normalize a float tensor image with mean and standard deviation.<br>
</p>

<p>
	To put all together, let's define a dictionary with all the transformations:
</p>

<div>
	<img src="/static/report_images/transforms1.PNG">
</div>
<small>Image 25: Transforms Dictionary </small>

<p>
	We finish it up by loading all the preprocessed images using torch.utils.data.DataLoader class, an iterable over a dataset.
	This way, we can fill our train, valid and load datasets.
</p>
<div>
	<img src="/static/report_images/dataloaders.PNG">
</div>
<small>Image 26: Dataloaders </small>

<p>
	Now the dataloaders are ready with all the images preprocessed, and we have access to the names of all dog classes within the dataset.
	It feels like all this work so far is starting to get us somewhere.
</p>
<div>
	<img src="/static/report_images/classes_names.PNG">
</div>
<small>Image 27: Classes sample </small>

<h3>- Step 5: Create a CNN to classify dog breeds from scratch</h3>

<p>
	To create the CNN, first we need to define the Constructor, the basic components of the network, which consists of several layers: the convolutional layers,
	the pooling layer, the dropout layer, and the full connected layers. So we will define a class which includes the methods defining all the
	components of a CNN network.
</p>

<div>
	<img src="/static/report_images/convnet1.PNG">
</div>
<small>Image 28: CNN Constructor </small>

<p>
	We are using:<br><br>
	Convolutional Layers - Five layers defined with respective 16, 32, 64, 128 and 256 filters, and all of them have kernel size = 3, stride = 1
	and padding = 1. <br>

	Pooling Layer - I picked method MaxPooling2D for the pooling layer. When added to a model, max pooling reduces the dimensionality of images
	by reducing the number of pixels in the output from the previous convolutional layer. <br>

	Dropout Layer - Dropout Layer with a probability of 2% to prevent overfitting.<br>

	Fully Connected Layers - 2 fully connected layers, the first has a 256 input, and output to 512, which is the input to the second FC layer,
	and its output is the number of classes n_classes.<br>
</p>

<p>
	And second, we need to define the forward function, which is how the model is going to be run, from input to output.
</p>

<div>
	<img src="/static/report_images/forward.PNG">
</div>
<small>Image 29: CNN Forward function </small>

<p>
	The forward function works as follows:<br><br>
	A Relu activation function is applied to each layer to make the network non-linear.<br>

	The view function is applyed to prevent explicit data copy.<br>

	The FC layers are connected.<br>
</p>

<p>
	Now we need to specify the loss function and optimizer:
</p>
<div>
	<img src="/static/report_images/loss.PNG">
</div>
<small>Image 30: Loss function and optimizer </small>

<p>
	And now we train and validate the model using the train function below:
</p>
<div>
	<img src="/static/report_images/train1.PNG">
</div>
<small>Image 31: Train and validate function </small>

<p>
	Since this project works with images, if there is no GPU available, it could be quite slow to do all the training and processing required.
	This notebook that I'm using to test the entire code was not GPU supported, so I only ran a few epochs to show the results, and then trained
	the model in a GPU environment with 100 epochs. This new model can be imported into our current project by using torch.load, an extremely
	useful tool that will also helps us to import the final model to our Flask application.
</p>

<div>
	<img src="/static/report_images/model_scratch.PNG">
</div>
<small>Image 32: Trained model and 100 epoch loaded model </small>

<p>
	To finish it up this step, we need a function to test the accuracy of our model_scratch:
</p>
<div>
	<img src="/static/report_images/test_modelscratch.PNG">
</div>
<small>Image 33: Testing model_scratch </small>

<p>
	43% accuracy is better than our established 40% goal mark, so this step was finished successfully.
</p>

<h3>- Step 6: Create a CNN to classify dog breeds using transfer learning</h3>

<p>
	We've already discussed the concept of transfer learning, so we can jump straight indo coding. For this step, we'll use
	DenseNet161 pretrained model. And we don't have to do the preprocess again because we'll use the same dataloaders defined
	on the previous step. We'll also use the same train and test functions. So all it has do be done is download the model,
	define the loss function and optmizer and then start training.
</p>
<div>
	<img src="/static/report_images/model_transfer.PNG">
</div>
<small>Image 34: Testing model_scratch </small>
<div>
	<img src="/static/report_images/loss_transfer.PNG">
</div>
<small>Image 35: Loss and optimizer model_transfer </small>

<p>
	If 100 epochs takes a long time to train with a CNN from scratch, imagine how long a CNN from transfer learning would take without.
	In the next cell I can give an estimate, just to run 1 epoch it took more than 2 hours. So, again, I opened up a GPU environment, trained
	an 100 epoch model_transfer and brought to the project.
</p>
<div>
	<img src="/static/report_images/modeltransfertrain.PNG">
</div>
<small>Image 36: model_transfer train </small>

<p>
	Using the same test function, we can see how well this model performed.
</p>
<div>
	<img src="/static/report_images/transfertest.PNG">
</div>
<small>Image 36: model_transfer test </small>

<p>
	As expected, model_transfer100 easily surpass model_scratch100. This shows how powerful transfer learning is. And also, 89% is a great accuracy,
	higher than our goal of 85%.
</p>

<p>
	Now, using model_transfer100, we can write a predict_breed_function that recieves an image input and returns the predicted breed of that image.
</p>
<div>
	<img src="/static/report_images/predictfunction.PNG">
</div>
<small>Image 37: Predict Breed function </small>

<p>
	Let's test it with a few images from the valid dog dataset:
</p>
<div>
	<img src="/static/report_images/predicttest.PNG">
</div>
<small>Image 37: Predict Breed function test </small>

<p>
	Looks pretty decent! With this function, we finish Step 6 of our project. And now it's time to put everything together.
</p>

<h3>- Step 7: Write the full algorithm</h3>
<p>
	It's been a long journey, but it’s finally reaching its climax. Now that we have the face_detector function, the dog_detector function,
	and the predict_breed_function, we can create the full code that will work based on these terms:<br><br>

	- Reads an input image;<br>
	- If a dog is detected in the image, return the predicted breed;<br>
	- If a human is detected in the image, return the resembling dog breed;<br>
	- If neither is detected in the image, return an error.<br><br>

	I added a few extra lines just to show the input image along with a sample image with the resulting predicted breed.
</p>

<div>
	<img src="/static/report_images/run_app1.PNG">
</div>
<small>Image 38: Final algorithm </small>

<p>
	Let's test it in a dog image from the dataset:
</p>
<div>
	<img src="/static/report_images/dog_app_test.PNG">
</div>
<small>Image 39: Testing the algorithm on a dog image </small>

<p>
	Let's test it in a human image from the dataset:
</p>
<div>
	<img src="/static/report_images/human_app_test.PNG">
</div>
<small>Image 40: Testing the algorithm on a human image </small>

<p>
	And let's try it in a image that should give an error:
</p>
<div>
	<img src="/static/report_images/strawberryerror.PNG">
</div>
<small>Image 41: Testing the algorithm on a error image </small>

<p>
	It's working like a charm! Now all we have to do is test it on images that are not in the dataset to check the results!.
</p>

<h2>Results: </h2>
<p>
	I've chosen 6 images to test it out:<br><br>

	- A picture of my dog (a mixed breed that contains a lot of German Shoirthaired Pointer);<br>
	- A picture of myself :)<br>
	- A picture of my friend's dog (another mixed breed with a lot of Labrador Retriever);<br>
	- A picture from a googled random woman;<br>
	- A picture of a cartoon dog;<br>
	- A picture of a cat;<br><br>

	Let's see what was produced by each image.
</p>

<div>
	<img src="/static/report_images/my_dog.PNG">
</div>
<small>Image 42: Testing the algorithm on my dog </small>
<br><br>

<div>
	<img src="/static/report_images/myself.PNG">
</div>
<small>Image 43: Testing the algorithm on myself </small>
<br><br>

<div>
	<img src="/static/report_images/my_friends.PNG">
</div>
<small>Image 44: Testing the algorithm on my friends dog </small>
<br><br>

<div>
	<img src="/static/report_images/rando.PNG">
</div>
<small>Image 45: Testing the algorithm on a random woman image </small>
<br><br>

<div>
	<img src="/static/report_images/cartoondog.PNG">
</div>
<small>Image 46: Testing the algorithm on a cartoon dog </small>
<br><br>

<div>
	<img src="/static/report_images/cat.PNG">
</div>
<small>Image 47: Testing the algorithm on a cat </small>
<br><br>

<p>
	After analyzing these results, I can say that the outputs were exactly what I expected. I tried two mixed breed dogs, mine which has a strong
	presence of a German Shorthaired Pointer, and another one from a friend of mine which has a lot of a Labrador Retriever, and those were exactly
	the results I got. For my personal picture, it was rather humourous that the output was a curly haired dog because of my curly-ish hair,
	and a simular result happened with the random woman picture, making me believe that the hair has a strong weight in the model output.
	As for the other two images, I tried a cartoon dog and a cat, and the algorithm gave errors for both images.
</p>

<p>
	This makes sence, because the index returned by the VGG16 model for these two pictures are "comic book" and "tiger cat", which are correct,
	but ignored by my application.
</p>

<h2> Deploy in a Flask Web Application: </h2>

<p>
	When we're studying Machine Learning, we are often overwhelmed by lots of tools, algorithms, techniques and possibilities, but we forget one of the
	most important steps: deployment. What is the point of working hard on a Machine Learning application if nobody gets to use it? So to finish it up this
	project, I'll launched a small web application using Flask, and I'll leave this entire reported accessible from there.
</p>

<p>
	In terms of Python programming there isn't too much secrets, we can use torch.load to load our model_transfer100 model, and all it needs to be done
	is redefine (or import) the same functions we did for the project, face_detector, dog_detector and predict_breed, and call these functions in specific
	routes of the application.
</p>
<p>
	There is also a few html and css magic to create and customize input buttons for the images, but nothing too complex. In this project Github, there is
	a folder with the Flask Application and instructions to run it properly.
</p>
<p>
	The final web site is hosted on
	<a href="http://andrevargas22.pythonanywhere.com">
		andrevargas22.pythonanywhere.com</a>
</p>

<p>
	I also had an ideia of coding a Twitter bot using Tweepy, a python library for Twitter API, but unfortunately, due to a tight schedule, I didn't have
	enough time to pull it off before the deadline. It's certainly a challenge that I'll try eventually.
</p>

<h2> Conclusion: </h2>
<p>
	This is was a very fun project to work on. It helped me understand a lot of concepts in the subject of Convolutional Neural Networks,
	Transfer Learning, Image Processing and even Flask Web Design, which is something I'm not used to do.
</p>

<p>
	There are a few points of improvement to this project:<br><br>

	1) There are a few breeds not classified on this pretrained model which will give an error. For example, a picture of a Shiba Inu (japanese dog breed)
	will return index 273 - Canis Dingo, a type of australian wild canid, and since it's not within the index range accepted by the algorithm, it will give
	an error. So perhaps the usage of a different model with more classified breeds and a larger dataset would fix this problem.<br><br>

	2) I was able to get my model work with transfer learning from DenseNet161, which worked very well, but I wish to have tested with some other models
	for comparison. I know that ResNet50, InceptionV3 and Xception are interesting options, and maybe one of this models has an even better perfomance then
	DenseNet161.<br><br>

	3) I didn't make too many tests with the hyperparameters, I used the same values in the documentation examples, so I believe there is probably
	some better tuning to do.<br><br>
</p>

<h2>References</h2>

<p>[1].
	<a href="https://www.collinsdictionary.com/dictionary/english/intelligence">Definition of Intelligence by Collins English Dictionary</a>.
</p>

<p>[2]. Image taken from
	"<a href="https://www.delish.com/food-news/a32475445/strawberry-hack-ice-water/">This Strawberry Hack Will Instantly Make
	Your Mushy Fruit as Good as New Again</a>"
</p>

<p>[3].
	<a href="https://www.akc.org/dog-breeds/">American Kennel Club: Dog Breeds</a>.
</p>

<p>[4]. Image taken from
	"<a href="https://www.akc.org/expert-advice/lifestyle/dog-breed-look-alikes/">American Kennel Club: Can You Tell These Dog Breed Look-Alikes Apart?</a>".
</p>

<p>[5]. Image taken from
	"<a href="https://hamamatsu.magnet.fsu.edu/articles/digitalimagebasics.html">Concepts in Digital Imaging Technology</a>".
</p>

<p>[6]. Image taken from
	"<a href="https://ai.stanford.edu/~syyeung/cvweb/tutorial1.html">Stanford AI Lab Tutorial 1: Image Filtering</a>".
</p>

<p>[7].
	<a href="https://towardsdatascience.com/training-convolutional-neural-networks-to-categorize-clothing-with-pytorch-30b6d399f05f">
		Training Convolutional Neural Networks to Categorize Clothing with Pytorch</a>.
</p>

<p>[8].
	<a href="https://docs.ecognition.com/v9.5.0/eCognition_documentation/Reference%20Book/23%20Convolutional%20Neural%20Network%20Algorithms/Convolutional%20Neural%20Network%20Algorithms.htm">
		Convolutional Neural Network Algorithms</a>.
</p>

<p>[9].
	<a href="https://www.flatworldsolutions.com/data-science/articles/7-applications-of-convolutional-neural-networks.php">7 Applications
		of Convolutional Neural Networks</a>
</p>

<p>[10]. Code taken from
	"<a href="https://machinelearningmastery.com/how-to-perform-face-detection-with-classical-and-deep-learning-methods-in-python-with-keras/">How to Perform Face
		Detection with Deep Learning</a>".
</p>

<p>[11]. Code taken from
	"<a href="https://towardsdatascience.com/dog-breed-classification-using-cnns-and-transfer-learning-e36259b29925">Applying transfer-learning
		in CNNs for dog breed classification</a>".
</p>